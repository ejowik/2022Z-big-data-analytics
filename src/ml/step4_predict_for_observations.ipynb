{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# !pip install pystan==2.19.1.1\n# !pip install fbprophet\n# !pip install plotly\n# !pip install --upgrade pretty-confusion-matrix --user\n\n# data path in VM local file system: home/data/in/ggtrends_downloads, home/data/in/weather_downloads\n# data path in HDFS: hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends, hdfs://cluster-bda4-m/user/root/project/data/in/weather\n\n# To upload data from VM local file system to HDFS execute command similar to the following one:\n# hdfs dfs -copyFromLocal home/data/in/ggtrends_downloads/Instagram_Hamburg_historic.csv hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n"}], "source": "from pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql.types import FloatType\n\nfrom fbprophet import Prophet\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nimport seaborn as sns\nimport time\n\nimport os\nimport ast\nimport re\nfrom datetime import datetime\nfrom pyspark.sql import SparkSession\nfrom concurrent.futures import TimeoutError\n\nfrom fbprophet.serialize import model_to_json, model_from_json\n\nfrom tqdm.notebook import tqdm"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# Parameters section\n\n# credentials_path = 'weather-based-forecasting-v2-c4bde37656a7.json'\n# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\ntimeout = 5.0\n\nmodel_variables = [\"Temperature\", \"Relative Humidity\", \"Wind Speed\", \"ds\"]"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<fbprophet.forecaster.Prophet object at 0x7f74aefc9090>\n"}], "source": "# Load pre-trained model from HDFS\n\nwith open('serialized_model.json', 'r') as fin:\n    m = model_from_json(fin.read())  # Load model (test)\n    \nprint(m)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Time series data analysis with Spark\") \\\n    .config(\"spark.redis.ssl\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "from google.cloud import pubsub_v1"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# Auxiliary functions\n\n\ndef trim_colnames(df):\n    colnames = [re.sub(\"[^a-zA-Z0-9,]\", \"\", i) for i in df.columns]\n    df.columns = colnames\n    return df\n\n\ndef assign_class(value, decision_boundaries):\n    for _range, _class in decision_boundaries.items():\n        # for every pair that you see in table\n        if _range[0] <= value < _range[1]:\n            return _class\n        \n        \ndef predict_class(model, df):\n    pred = model.predict(df.toPandas())\n    decision_boundaries = {\n        (-np.inf, -pred.yhat.quantile(0.9)): -1,\n        (-pred.yhat.quantile(0.9), pred.yhat.quantile(0.9)): 0,\n        (pred.yhat.quantile(0.9), np.inf): 1,\n    }\n\n    pred['pred_class'] = pred.yhat.apply((lambda x: assign_class(x, decision_boundaries)))\n    \n    return pred[['ds', 'pred_class']]\n"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "weather_batch = []\n\ndef weather_callback(message):\n\n    print(f'Received weather message')\n    message.ack()\n    obs = ast.literal_eval(message.data.decode('UTF-8'))\n    obs_df = pd.DataFrame.from_dict([obs])\n    \n    # Process an item of stream data (e.g., forecast data)\n\n    obs_df[\"ds\"] = pd.to_datetime(obs_df[\"Date time\"])\n    obs_df = obs_df[model_variables]\n    obs_df = trim_colnames(obs_df)\n    \n    obs_DF = spark.createDataFrame(obs_df)\n    \n    # Real-time prediction\n    # transforming a regression problem into a classification one\n    pred = predict_class(m, obs_DF)\n    \n    weather_batch.append(pred)\n\n    # Saving prediction in HDFS\n    ts = str(datetime.now().timestamp()).replace(\".\", \"-\")\n    path = f\"hdfs://cluster-bda2-m/user/root/predictions/observations/{ts}.parquet\"\n\n    spark.createDataFrame(pred).write.parquet(path)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_obs_sub\nReceived weather message\nReceived weather message\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_obs_sub\nReceived weather message\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_obs_sub\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "No new messages - check number: 3.\n"}], "source": "check_count = 0\nweather_batch = []\nprev_length = len(weather_batch)\n\nmax_check = 3\n\nwhile True:\n    if check_count >= max_check: break\n    \n    # Initialize subscriber\n    subscriber = pubsub_v1.SubscriberClient()\n\n    # Weather\n    subscription_path_WD = 'projects/weather-based-forecasting-v2/subscriptions/python_obs_sub'\n    streaming_pull_future_WD = subscriber.subscribe(\n        subscription_path_WD, callback=weather_callback\n    )\n    print(f'Listening for messages on {subscription_path_WD}')\n\n    try:\n        streaming_pull_future_WD.result(timeout=120)  # wrap subscriber in a 'with' block to automatically call close() when done\n    except TimeoutError:        \n        streaming_pull_future_WD.cancel()  # trigger the shutdown\n        streaming_pull_future_WD.result()  # block until the shutdown is complete\n        \n    if len(weather_batch) > prev_length: prev_length = len(weather_batch)\n    else:\n        check_count += 1\n        print(f\"No new messages - check number: {check_count}.\")\n        time.sleep(60)\n"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "1\n"}], "source": "print(1)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Initialize subscriber\n# subscriber = pubsub_v1.SubscriberClient()\n\n# # Weather\n# subscription_path_WD = 'projects/weather-based-forecasting-v2/subscriptions/python_obs_sub'\n# streaming_pull_future_WD = subscriber.subscribe(\n#     subscription_path_WD, callback=weather_callback\n# )\n# print(f'Listening for messages on {subscription_path_WD}')"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# # Reading messages from topics using multiple subscribers\n# weather_batch = []\n\n# with subscriber:  # wrap subscriber in a 'with' block to automatically call close() when done\n#     try:\n#         streaming_pull_future_WD.result(timeout=120)\n#     except TimeoutError:        \n#         streaming_pull_future_WD.cancel()  # trigger the shutdown\n#         streaming_pull_future_WD.result()  # block until the shutdown is complete"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}