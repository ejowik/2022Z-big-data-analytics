{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Estymacja pierwszego modelu i zapis w HDFS"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# Prerequisites\n\n# !pip install pystan==2.19.1.1\n# !pip install fbprophet\n# !pip install plotly\n# !pip install --upgrade pretty-confusion-matrix --user"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n"}], "source": "from pyspark.sql import SparkSession\nfrom fbprophet import Prophet\n\nimport numpy as np\nimport pandas as pd\nfrom typing import List\n\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nimport seaborn as sns\nfrom pretty_confusion_matrix import pp_matrix\n\nfrom fbprophet.serialize import model_to_json, model_from_json"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "# Auxiliary functions\n\ndef load_parquet_from_HDFS(spark, fpath: List[str]):\n    df = spark.read.parquet(fpath)\n\n    return df\n\n\ndef estimate_prophet(df_train, date_col, y_col):\n    regressors = list(set(df_train.columns) - set([date_col, y_col]))\n    m = Prophet()\n\n    for c in regressors:\n        m.add_regressor(c)\n\n    m.fit(df_train.toPandas())\n    return m\n\n\ndef symmetric_mean_absolute_percentage_error(A, F):\n    with np.errstate(divide='ignore', invalid='ignore'):\n        tmp = 2 * np.abs(F-A) / (np.abs(A) + np.abs(F))\n    tmp[np.isnan(tmp)] = 0\n    return np.sum(tmp) / len(tmp) * 100"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# Parameters settings\n\ntrain_fpath = \"hdfs://cluster-bda2-m/user/root/modeling/in/df_day_train_scaled.parquet\"\n\ndate_col = 'ds'\ny_col = 'y'"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Time series data analysis with Spark\") \\\n    .config(\"spark.redis.ssl\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "# load data from HDFS\ndf_train = load_parquet_from_HDFS(spark, train_fpath)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "INFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n"}], "source": "# estimate regression model\nmodel = estimate_prophet(df_train, date_col, y_col)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "with open('serialized_model.json', 'w') as fout:\n    fout.write(model_to_json(model))  # Save model"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<fbprophet.forecaster.Prophet object at 0x7f718b494710>\n"}], "source": "# Test\n\nwith open('serialized_model.json', 'r') as fin:\n    m = model_from_json(fin.read())  # Load model (test)\n    \nprint(m)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}