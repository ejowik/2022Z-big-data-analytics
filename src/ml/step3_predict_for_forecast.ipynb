{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# !pip install pystan==2.19.1.1\n# !pip install fbprophet\n# !pip install plotly\n# !pip install --upgrade pretty-confusion-matrix --user\n\n# data path in VM local file system: home/data/in/ggtrends_downloads, home/data/in/weather_downloads\n# data path in HDFS: hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends, hdfs://cluster-bda4-m/user/root/project/data/in/weather\n\n# To upload data from VM local file system to HDFS execute command similar to the following one:\n# hdfs dfs -copyFromLocal home/data/in/ggtrends_downloads/Instagram_Hamburg_historic.csv hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": "from pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql.types import FloatType\n\nfrom fbprophet import Prophet\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nimport seaborn as sns\nimport time\n\nimport os\nimport ast\nimport re\nfrom datetime import datetime\nfrom pyspark.sql import SparkSession\nfrom concurrent.futures import TimeoutError\n\nfrom fbprophet.serialize import model_to_json, model_from_json\n\nfrom tqdm.notebook import tqdm"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "# Parameters section\n\n# credentials_path = 'weather-based-forecasting-v2-c4bde37656a7.json'\n# os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path\ntimeout = 5.0\n\nmodel_variables = [\"Temperature\", \"Relative Humidity\", \"Wind Speed\", \"ds\"]"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "<fbprophet.forecaster.Prophet object at 0x7f4f0ae1e410>\n"}], "source": "# Load pre-trained model from HDFS\n\nwith open('serialized_model.json', 'r') as fin:\n    m = model_from_json(fin.read())  # Load model (test)\n    \nprint(m)"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Time series data analysis with Spark\") \\\n    .config(\"spark.redis.ssl\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": "from google.cloud import pubsub_v1"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "# Auxiliary functions\n\n\ndef trim_colnames(df):\n    colnames = [re.sub(\"[^a-zA-Z0-9,]\", \"\", i) for i in df.columns]\n    df.columns = colnames\n    return df\n\n\ndef assign_class(value, decision_boundaries):\n    for _range, _class in decision_boundaries.items():\n        # for every pair that you see in table\n        if _range[0] <= value < _range[1]:\n            return _class\n        \n        \ndef predict_class(model, df):\n    pred = model.predict(df.toPandas())\n    decision_boundaries = {\n        (-np.inf, -pred.yhat.quantile(0.9)): -1,\n        (-pred.yhat.quantile(0.9), pred.yhat.quantile(0.9)): 0,\n        (pred.yhat.quantile(0.9), np.inf): 1,\n    }\n\n    pred['pred_class'] = pred.yhat.apply((lambda x: assign_class(x, decision_boundaries)))\n    \n    return pred[['ds', 'pred_class']]\n"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "weather_batch = []\n\ndef weather_callback(message):\n\n    print(f'Received weather message')\n    message.ack()\n    weather_batch.append(message.data)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub\nReceived weather message\nReceived weather message\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9d82a6e32346433d8c91cb70f9d0af3c", "version_major": 2, "version_minor": 0}, "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]"}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "dc1b3a81a1c44dffb827cdae5a5ee2db", "version_major": 2, "version_minor": 0}, "text/plain": "  0%|          | 0/720 [00:00<?, ?it/s]"}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "991c94ccfe4744c2aed704544adc5e62", "version_major": 2, "version_minor": 0}, "text/plain": "  0%|          | 0/720 [00:00<?, ?it/s]"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub\nReceived weather message\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "42b43b04afb949fc9b3e765ad455328f", "version_major": 2, "version_minor": 0}, "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]"}, "metadata": {}, "output_type": "display_data"}, {"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "9e01a9a3f4dc4ba49b8cf3f0424ce5ca", "version_major": 2, "version_minor": 0}, "text/plain": "  0%|          | 0/720 [00:00<?, ?it/s]"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Listening for messages on projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "No new messages - check number: 2.\nListening for messages on projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "No new messages - check number: 3.\nListening for messages on projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub\n"}, {"name": "stderr", "output_type": "stream", "text": "INFO:google.api_core.bidi:Thread-ConsumeBidirectionalStream exiting\n"}, {"name": "stdout", "output_type": "stream", "text": "No new messages - check number: 4.\n"}], "source": "check_count = 0\nweather_batch = []\nprev_length = len(weather_batch)\n\nmax_check = 3\n\nwhile True:\n    if check_count >= max_check: break\n    \n    # Initialize subscriber\n    subscriber = pubsub_v1.SubscriberClient()\n\n    # Weather\n    subscription_path_WD = 'projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub'\n    streaming_pull_future_WD = subscriber.subscribe(\n        subscription_path_WD, callback=weather_callback\n    )\n    print(f'Listening for messages on {subscription_path_WD}')\n\n    try:\n        streaming_pull_future_WD.result(timeout=120)  # wrap subscriber in a 'with' block to automatically call close() when done\n    except TimeoutError:        \n        streaming_pull_future_WD.cancel()  # trigger the shutdown\n        streaming_pull_future_WD.result()  # block until the shutdown is complete\n        \n    if len(weather_batch) > prev_length:\n        \n        cnt_diff = len(weather_batch) - prev_length\n        \n        new_batches = weather_batch[-cnt_diff:]\n        \n        batch_df = pd.DataFrame()\n\n        for weather_obs in tqdm(new_batches):\n            batch_dict = ast.literal_eval(weather_obs.decode('UTF-8'))\n            batch_obs_list = list(batch_dict.values())\n            for row in tqdm(batch_obs_list):\n                row_df = pd.DataFrame([row])\n\n                row_df[\"ds\"] = pd.to_datetime(row_df[\"Date time\"])\n                row_df = row_df[model_variables]\n                row_df = trim_colnames(row_df)\n\n                batch_df = pd.concat([batch_df, row_df]).reset_index(drop=True)\n\n        # conversion to Spark dataframe\n        batch_DF = spark.createDataFrame(batch_df)\n\n        # Prediction\n\n        # transforming a regression problem into a classification one\n        pred = predict_class(m, batch_DF)\n\n        # Saving prediction in HDFS\n        ts = str(datetime.now().timestamp()).replace(\".\", \"-\")\n        path = f\"hdfs://cluster-bda2-m/user/root/predictions/forecasts/{ts}.parquet\"\n\n        spark.createDataFrame(pred).write.parquet(path)\n        \n        # Update\n        prev_length = len(weather_batch)\n    else:\n        check_count += 1\n        print(f\"No new messages - check number: {check_count}.\")\n        time.sleep(60)\n"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "# # Initialize subscriber\n# subscriber = pubsub_v1.SubscriberClient()\n\n# # Weather\n# subscription_path_WD = 'projects/weather-based-forecasting-v2/subscriptions/python_forecast_sub'\n# streaming_pull_future_WD = subscriber.subscribe(\n#     subscription_path_WD, callback=weather_callback\n# )\n# print(f'Listening for messages on {subscription_path_WD}')"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": "# # Reading messages from topics using multiple subscribers\n# weather_batch = []\n\n# with subscriber:\n#     try:\n#         streaming_pull_future_WD.result(timeout=120)  # wrap subscriber in a 'with' block to automatically call close() when done\n#     except TimeoutError:        \n#         streaming_pull_future_WD.cancel()  # trigger the shutdown\n#         streaming_pull_future_WD.result()  # block until the shutdown is complete"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": "# # Process a batch of stream data (e.g., forecast data)\n# # TODO: bie\u017c\u0105ce trackowanie d\u0142ugo\u015bci listy wiadomo\u015bci i powtarzanie procesu przy zmianie\n\n# batch_df = pd.DataFrame()\n\n# for weather_obs in tqdm(weather_batch):\n#     batch_dict = ast.literal_eval(weather_obs.decode('UTF-8'))\n#     batch_obs_list = list(batch_dict.values())\n#     for row in tqdm(batch_obs_list):\n#         row_df = pd.DataFrame([row])\n        \n#         row_df[\"ds\"] = pd.to_datetime(row_df[\"Date time\"])\n#         row_df = row_df[model_variables]\n#         row_df = trim_colnames(row_df)\n        \n#         batch_df = pd.concat([batch_df, row_df]).reset_index(drop=True)\n        \n# # conversion to Spark dataframe\n# batch_DF = spark.createDataFrame(batch_df)"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": "# # Prediction\n\n# # transforming a regression problem into a classification one\n# pred = predict_class(m, batch_DF)\n\n# # Saving prediction in HDFS\n# ts = str(datetime.now().timestamp()).replace(\".\", \"-\")\n# path = f\"hdfs://cluster-bda2-m/user/root/predictions/forecasts/{ts}.parquet\"\n    \n# spark.createDataFrame(pred).write.parquet(path)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}