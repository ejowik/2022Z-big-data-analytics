{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "# Preprocessing danych historycznych i zapis w HDFS\n\n# hdfs dfs -copyFromLocal etc/jupyter/symlinks_for_jupyterlab_widgets/Local%20Disk/home/jowike/weather hdfs://cluster-bda2-m/user/root/history"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "from typing import List\nfrom pyspark.sql import SparkSession\nfrom functools import reduce\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql.types import FloatType\nimport re "}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "# Auxiliary functions\n\ndef load_weather_batches(spark, fpaths: List[str]):\n    batches = []\n\n    for path in fpaths:\n        batch_df = spark.read.csv(path, inferSchema=\"true\", header=\"true\")\n        batches.append(batch_df)\n\n    weather_df = reduce(DataFrame.unionAll, batches)\n    weather_df = weather_df.withColumnRenamed(\"Date time\", \"date\")\n    weather_df = weather_df.withColumn(\n        \"date\", F.to_timestamp(F.col(\"date\"), \"MM/dd/yyyy HH:mm:ss\").alias(\"date\")\n    )\n    return weather_df\n\n\ndef load_ggtrends(spark, path):\n    _website_df = spark.read.csv(path, inferSchema=\"true\", header=\"true\")\n    website_df = _website_df.select(\n        F.to_timestamp(F.col(\"date\"), \"MM-dd-yyyy\").alias(\"date\"),\n        F.col(\"value\").alias(\"y\"),\n    )\n    return website_df\n\n\ndef merge_sources(weather_df, website_df):\n    columns = [\n        \"date\",\n        \"Address\",\n        \"Temperature\",\n        \"Relative Humidity\",\n        \"Wind Speed\",\n        \"Visibility\",\n        \"Cloud Cover\",\n        \"y\",\n    ]\n\n    df = weather_df.join(website_df, how=\"inner\", on=\"date\").select(*columns)\n    partition = Window.partitionBy(\"Address\").orderBy(F.col(\"date\").asc())\n    df = df.withColumn(\"lag1\", F.lag(\"y\", 1).over(partition))\n    df = (\n        df.withColumn(\"lag24\", F.lag(\"y\", 24).over(partition))\n        .drop(\"Address\")\n        .na.drop(subset=[\"lag1\", \"lag24\"])\n        .na.fill(0)\n    )\n\n    _df = df.withColumn(\n        \"filter_col\",\n        F.when(\n            F.date_format(F.col(\"date\"), \"HH:mm:ss\").between(\"07:00:00\", \"23:00:00\"),\n            \"day\",\n        ).otherwise(\"night\"),\n    )\n    return _df\n\n\ndef split_days_and_nights(df):\n    df_day = df.filter(F.col(\"filter_col\") == \"day\").drop(\"filter_F.col\")\n    df_night = df.filter(F.col(\"filter_col\") == \"night\").drop(\"filter_F.col\")\n    return df_day, df_night\n\n\ndef train_test_split(df, columns):\n    df = df.withColumn(\n        \"rank\", F.percent_rank().over(Window.partitionBy().orderBy(\"date\"))\n    )\n    df_train = df.where(\"rank <= .8\").select(*columns).sort(\"date\")\n    df_test = df.where(\"rank > .8\").select(*columns).sort(\"date\")\n    return df_train, df_test\n\n\ndef scale_df(df, columns_to_scale):\n    assemblers = [\n        VectorAssembler(inputCols=[col], outputCol=col + \"_vec\")\n        for col in columns_to_scale\n    ]\n    scalers = [\n        MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\")\n        for col in columns_to_scale\n    ]\n\n    pipeline = Pipeline(stages=assemblers + scalers)\n\n    scalerModel = pipeline.fit(df)\n    _df_scaled = scalerModel.transform(df)\n    names = {x + \"_scaled\": x for x in columns_to_scale}\n\n    firstelement = F.udf(lambda v: float(v[0]), FloatType())\n    columns_to_select = [F.col(\"date\"), F.col(\"y\")] + [\n        firstelement(c).alias(c) for c in names.keys()\n    ]\n    _df_scaled = _df_scaled.select(columns_to_select)\n\n    names[\"date\"] = \"ds\"\n    names[\"y\"] = \"y\"\n\n    df_scaled = _df_scaled.select([F.col(c).alias(names[c]) for c in names.keys()])\n    return df_scaled\n\n\ndef trim_colnames(df1):\n    schema1 = [re.sub(\"[^a-zA-Z0-9,]\", \"\", i) for i in df1.columns] \n    df2 = df1.toDF(*schema1)\n    return df2"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "# Parameters settings\n\nweather_batches = [\n    \"hdfs://cluster-bda2-m/user/root/history/weather/merged_historic_Hamburg_batch1.csv\",\n    \"hdfs://cluster-bda2-m/user/root/history/weather/merged_historic_Hamburg_batch2.csv\",\n    \"hdfs://cluster-bda2-m/user/root/history/weather/merged_historic_Hamburg_batch3.csv\",\n    \"hdfs://cluster-bda2-m/user/root/history/weather/merged_historic_Hamburg_batch4.csv\"\n]\n\nggtrends_fpath = \"hdfs://cluster-bda2-m/user/root/history/ggtrends/Instagram_Hamburg_historic.csv\"\n\ncolumns = [\n    \"date\",\n    \"Temperature\",\n    \"Relative Humidity\",\n    \"Wind Speed\",\n    \"Visibility\",\n    \"lag1\",\n    \"lag24\",\n    \"y\"]\n\ncolumns_to_scale = columns[1:-1]"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Time series data analysis with Spark\") \\\n    .config(\"spark.redis.ssl\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# Reading historical data from HDFS\nweather_df = load_weather_batches(spark, weather_batches)\nggtrends_df = load_ggtrends(spark, ggtrends_fpath)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "# Source data integration\ndf = merge_sources(weather_df, ggtrends_df)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "# Day and night data separation\ndf_day, df_night = split_days_and_nights(df)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "# Train, test split\ndf_day_train, df_day_test = train_test_split(df_day, columns)\ndf_night_train, df_night_test = train_test_split(df_night, columns)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "# Scaling\ndf_day_train_scaled = scale_df(df_day_train, columns_to_scale)\ndf_day_test_scaled = scale_df(df_day_test, columns_to_scale)\ndf_night_train_scaled = scale_df(df_night_train, columns_to_scale)\ndf_night_test_scaled = scale_df(df_night_test, columns_to_scale)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "trim_colnames(df_day_train_scaled).write.mode(\"overwrite\").parquet(\n    \"hdfs://cluster-bda2-m/user/root/modeling/in/df_day_train_scaled.parquet\"\n)\ntrim_colnames(df_day_test_scaled).write.mode(\"overwrite\").parquet(\n    \"hdfs://cluster-bda2-m/user/root/modeling/in/df_day_test_scaled.parquet\"\n)\ntrim_colnames(df_night_train_scaled).write.mode(\"overwrite\").parquet(\n    \"hdfs://cluster-bda2-m/user/root/modeling/in/df_night_train_scaled.parquet\"\n)\ntrim_colnames(df_night_test_scaled).write.mode(\"overwrite\").parquet(\n    \"hdfs://cluster-bda2-m/user/root/modeling/in/df_night_test_scaled.parquet\"\n)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}