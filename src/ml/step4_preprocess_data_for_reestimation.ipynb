{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "import os\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom functools import reduce\nfrom datetime import datetime\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import MinMaxScaler\nfrom pyspark.sql.types import FloatType\n\nos.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:2.4.8 pyspark-shell'"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# Auxiliary functions\n\n\ndef load_parquet_from_HDFS(spark, fpath):\n    return spark.read.parquet(fpath)\n\n\ndef load_avro_from_HDFS(spark, fpath):\n    return spark.read.format(\"avro\").load(fpath)\n\n\ndef load_weather_obs(spark, weather_fpath):\n    # process weather data\n    _weather_df = load_avro_from_HDFS(spark, weather_fpath)\n    weather_cmask = [\"Date_time\"] + [c for c in _weather_df.columns if (LOCATION in c)]\n    _weather_df = _weather_df.select(weather_cmask)\n    _weather_df = _weather_df.withColumnRenamed(\"Date_time\", \"date\")\n    _weather_df = _weather_df.withColumn(\n        \"date\", F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"date\")\n    )\n\n    oldColumns = _weather_df.columns\n    newColumns = [c.replace(LOCATION, \"\").replace(\"_\", \"\") for c in oldColumns]\n\n    weather_df = reduce(\n        lambda _weather_df, idx: _weather_df.withColumnRenamed(\n            oldColumns[idx], newColumns[idx]\n        ),\n        range(len(oldColumns)),\n        _weather_df,\n    )\n\n    cols = REGRESSORS + [DT_COLUMN]\n\n    return weather_df.select(cols)\n\n\ndef load_ggtrends_obs(spark, ggtrends_fpath):\n    _ggtrends_df = load_avro_from_HDFS(spark, ggtrends_fpath)\n    ggtrends_cmask = [\"date\"] + [c for c in _ggtrends_df.columns if (LOCATION in c)]\n    _ggtrends_df = _ggtrends_df.select(ggtrends_cmask)\n    oldColumns = _ggtrends_df.columns\n    newColumns = [c.replace(LOCATION, \"\").replace(\"_\", \"\") for c in oldColumns]\n    ggtrends_df = reduce(\n        lambda _ggtrends_df, idx: _ggtrends_df.withColumnRenamed(\n            oldColumns[idx], newColumns[idx]\n        ),\n        range(len(oldColumns)),\n        _ggtrends_df,\n    )\n    return ggtrends_df.select(\n        F.to_timestamp(F.col(\"date\"), \"yyyy-MM-dd HH:mm:ss\").alias(DT_COLUMN),\n        F.col(WEBSITE).alias(\"y\"),\n    )\n\n\ndef merge_sources(weather_df, website_df):\n    columns = [\n        \"date\",\n        \"Temperature\",\n        \"RelativeHumidity\",\n        \"WindSpeed\",\n        \"y\",\n    ]\n\n    df = weather_df.join(website_df, how=\"inner\", on=\"date\").select(*columns)\n\n    return df\n\n\ndef __unify_dtypes(df, ref_df):\n    select_expr = [\n        F.col(c).cast(t) for c, t in ref_df.dtypes\n    ]\n\n    return df.select(*select_expr)\n\n\ndef extend_mod_df(mod_df, new_obs):\n    new_obs = __unify_dtypes(df=new_obs, ref_df=mod_df)\n    _df = mod_df.unionByName(new_obs)\n    df = _df.withColumn(\n        \"filter_col\",\n        F.when(\n            F.date_format(F.col(\"date\"), \"HH:mm:ss\").between(\"07:00:00\", \"23:00:00\"),\n            \"day\",\n        ).otherwise(\"night\"),\n    )\n    \n    return df\n\n\ndef split_days_and_nights(df):\n    df_day = df.filter(F.col(\"filter_col\") == \"day\").drop(\"filter_F.col\")\n    df_night = df.filter(F.col(\"filter_col\") == \"night\").drop(\"filter_F.col\")\n    return df_day, df_night\n\n\ndef train_test_split(df, columns):\n    df = df.withColumn(\n        \"rank\", F.percent_rank().over(Window.partitionBy().orderBy(\"date\"))\n    )\n    df_train = df.where(\"rank <= .8\").select(*columns).sort(\"date\")\n    df_test = df.where(\"rank > .8\").select(*columns).sort(\"date\")\n    return df_train, df_test\n\n\ndef scale_df(df, columns_to_scale):\n    assemblers = [\n        VectorAssembler(inputCols=[col], outputCol=col + \"_vec\")\n        for col in columns_to_scale\n    ]\n    scalers = [\n        MinMaxScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\")\n        for col in columns_to_scale\n    ]\n\n    pipeline = Pipeline(stages=assemblers + scalers)\n\n    scalerModel = pipeline.fit(df)\n    _df_scaled = scalerModel.transform(df)\n    names = {x + \"_scaled\": x for x in columns_to_scale}\n\n    firstelement = F.udf(lambda v: float(v[0]), FloatType())\n    columns_to_select = [F.col(\"date\"), F.col(\"y\")] + [\n        firstelement(c).alias(c) for c in names.keys()\n    ]\n    _df_scaled = _df_scaled.select(columns_to_select)\n\n    names[\"date\"] = \"ds\"\n    names[\"y\"] = \"y\"\n\n    df_scaled = _df_scaled.select([F.col(c).alias(names[c]) for c in names.keys()])\n    return df_scaled\n\n\ndef get_latest_train_details(root_dir=\"hdfs://cluster-bda2-m/user/root\"):\n    ls = !hdfs dfs -ls modeling/in\n    avail_files = [c for c in ls if f\"{str.lower(WEBSITE)}_{str.lower(LOCATION)}_ver\" in c]\n\n    files_dict = {}\n\n    for f in avail_files:\n        date, time, name = f.split()[-3:]\n        creation_timestamp = datetime.strptime(\" \".join([date, time]), \"%Y-%m-%d %H:%M\")\n\n        files_dict[name] = creation_timestamp\n    \n    newest_fname = max(files_dict, key=files_dict.get)\n\n    return os.path.join(root_dir, newest_fname), files_dict[newest_fname]\n\n\ndef get_version():\n    ls = !hdfs dfs -ls modeling/in\n    existing_versions = [c for c in ls if f\"{str.lower(WEBSITE)}_{str.lower(LOCATION)}_ver\" in c]\n    return len(existing_versions)+1\n\n\ndef get_new_ggtrends(ref_date):\n    ls = !hdfs dfs -ls ggtrends\n\n    avail_files = [c for c in ls if \"ggtrends_obs\" in c]\n\n    files_dict = {}\n\n    for f in avail_files:\n        date, time, name = f.split()[-3:]\n        creation_timestamp = datetime.strptime(\" \".join([date, time]), \"%Y-%m-%d %H:%M\")\n\n        files_dict[name] = creation_timestamp\n\n    new_obs = []\n\n    for (key, value) in files_dict.items():\n       # Check if key is even then add pair to new dictionary\n       if value < ref_date:  # TODO: replace '<' with '>'\n            new_obs.append(key)\n    return new_obs\n\n\ndef get_new_weather_obs(ref_date):\n    ls = !hdfs dfs -ls weather\n\n    avail_files = [c for c in ls if \"weather_obs\" in c]\n\n    files_dict = {}\n\n    for f in avail_files:\n        date, time, name = f.split()[-3:]\n        creation_timestamp = datetime.strptime(\" \".join([date, time]), \"%Y-%m-%d %H:%M\")\n\n        files_dict[name] = creation_timestamp\n\n    new_obs = []\n\n    for (key, value) in files_dict.items():\n       # Check if key is even then add pair to new dictionary\n       if value < ref_date:  # TODO: replace '<' with '>'\n            new_obs.append(key)\n    return new_obs\n"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "# Parameters section\n\nLOCATION = \"Hamburg\"\nWEBSITE = \"Instagram\"\nREGRESSORS = [\"Temperature\", \"RelativeHumidity\", \"WindSpeed\"]\nTARGET_COLUMN = \"y\"\nDT_COLUMN = \"date\"\n\ncolumns = [\n    \"date\",\n    \"Temperature\",\n    \"RelativeHumidity\",\n    \"WindSpeed\",\n    \"y\"]\n\ncolumns_to_scale = columns[1:-1]"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "spark = SparkSession \\\n    .builder \\\n    .appName(\"Time series data analysis with Spark\") \\\n    .config(\"spark.redis.ssl\", \"true\") \\\n    .getOrCreate()"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": "# Getting latest files paths\nprev_mod_df_fpath, prev_mod_df_creat_dt = get_latest_train_details()\nggtrends_fpaths = get_new_ggtrends(prev_mod_df_creat_dt)\nweather_fpaths = get_new_weather_obs(prev_mod_df_creat_dt)"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": "# Data loading\nprev_mod_frame = load_parquet_from_HDFS(spark, prev_mod_df_fpath)\nprev_mod_frame = prev_mod_frame.select(columns)\n\nweather_df = load_weather_obs(spark, weather_fpaths)\nggtrends_df = load_ggtrends_obs(spark, ggtrends_fpaths)"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- date: integer (nullable = true)\n |-- Temperature: string (nullable = true)\n |-- RelativeHumidity: string (nullable = true)\n |-- WindSpeed: string (nullable = true)\n |-- y: string (nullable = true)\n\n"}], "source": "# TEMPORARY: overwrite column for merge\nfrom pyspark.sql.window import Window  # TODO: delete\n\nw = Window().orderBy(F.lit('A'))  # TODO: delete\n__weather_df = weather_df.drop(\"date\").withColumn(\"date\", F.row_number().over(w))  # TODO: delete\n__ggtrends_df = ggtrends_df.drop(\"date\").withColumn(\"date\", F.row_number().over(w))  # TODO: delete\n\n# Merge Google Trends and Weather observed values\nobs_df = merge_sources(__weather_df, __ggtrends_df)  # TODO: replace __weather_df, __ggtrends_df with weather_df, ggtrends_df\nobs_df.printSchema()\n\n# TEMPORARY: restore column after merge\nobs_df = obs_df.drop(\"date\").join(ggtrends_df, how=\"inner\", on=\"y\")  # TODO: delete"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "merged_df = extend_mod_df(mod_df = prev_mod_frame, new_obs=obs_df)"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": "# Save for further reestimation\nversion = get_version()\n\nmerged_df.select(prev_mod_frame.columns).write.mode(\"overwrite\").parquet(\n    f\"hdfs://cluster-bda2-m/user/root/modeling/in/{str.lower(WEBSITE)}_{str.lower(LOCATION)}_ver{version}.parquet\"\n)"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": "# Day and night data separation\ndf_day, df_night = split_days_and_nights(merged_df)"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": "# Train, test split\ndf_day_train, df_day_test = train_test_split(df_day, columns)\ndf_night_train, df_night_test = train_test_split(df_night, columns)"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": "# Scaling\ndf_day_train_scaled = scale_df(df_day_train, columns_to_scale)\ndf_day_test_scaled = scale_df(df_day_test, columns_to_scale)\ndf_night_train_scaled = scale_df(df_night_train, columns_to_scale)\ndf_night_test_scaled = scale_df(df_night_test, columns_to_scale)"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# Save data in HDFS\ndf_day_train_scaled.write.mode(\"overwrite\").parquet(\n    f\"hdfs://cluster-bda2-m/user/root/modeling/in/{str.lower(WEBSITE)}_{str.lower(LOCATION)}_day_train_ver{version}.parquet\"\n)\ndf_day_test_scaled.write.mode(\"overwrite\").parquet(\n    f\"hdfs://cluster-bda2-m/user/root/modeling/in/{str.lower(WEBSITE)}_{str.lower(LOCATION)}_day_test_ver{version}.parquet\"\n)\ndf_night_train_scaled.write.mode(\"overwrite\").parquet(\n    f\"hdfs://cluster-bda2-m/user/root/modeling/in/{str.lower(WEBSITE)}_{str.lower(LOCATION)}_night_train_ver{version}.parquet\"\n)\ndf_night_test_scaled.write.mode(\"overwrite\").parquet(\n    f\"hdfs://cluster-bda2-m/user/root/modeling/in/{str.lower(WEBSITE)}_{str.lower(LOCATION)}_night_test_ver{version}.parquet\"\n)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}