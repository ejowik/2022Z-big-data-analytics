{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pystan==2.19.1.1\n",
    "# !pip install fbprophet\n",
    "# !pip install plotly\n",
    "\n",
    "# data path in VM local file system: home/data/in/ggtrends_downloads, home/data/in/weather_downloads\n",
    "# data path in HDFS: hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends, hdfs://cluster-bda4-m/user/root/project/data/in/weather\n",
    "\n",
    "# To upload data from VM local file system to HDFS execute command similar to the following one:\n",
    "# hdfs dfs -copyFromLocal home/data/in/ggtrends_downloads/Instagram_Hamburg_historic.csv hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from fbprophet import Prophet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_mean_absolute_percentage_error(A, F):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        tmp = 2 * np.abs(F-A) / (np.abs(A) + np.abs(F))\n",
    "    tmp[np.isnan(tmp)] = 0\n",
    "    return np.sum(tmp) / len(tmp) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Time series data analysis with Spark\") \\\n",
    "    .config(\"spark.redis.ssl\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.read.csv(\"hdfs://cluster-bda4-m/user/root/project/data/in/weather/merged_historic_Hamburg.csv\", inferSchema=\"true\", header=\"true\")\n",
    "website_df = spark.read.csv(\"hdfs://cluster-bda4-m/user/root/project/data/in/ggtrends/Instagram_Hamburg_historic.csv\", inferSchema=\"true\", header=\"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather_df = weather_df.withColumnRenamed(\"Date time\", \"date\")\n",
    "# weather_df = weather_df.withColumn(\"date\", F.to_timestamp(F.col(\"date\"),\"MM/dd/yyyy HH:mm:ss\").alias(\"date\"))\n",
    "\n",
    "# website_df = website_df.select(F.to_timestamp(F.col(\"date\"),\"MM-dd-yyyy\").alias(\"date\"), F.col(\"value\").alias(\"y\"))\n",
    "\n",
    "\n",
    "# columns = [\n",
    "#     \"date\",\n",
    "#     \"Address\",\n",
    "#     \"Temperature\",\n",
    "#     \"Relative Humidity\",\n",
    "#     \"Wind Speed\",\n",
    "#     \"Visibility\",\n",
    "#     \"Cloud Cover\",\n",
    "#     \"y\"]\n",
    "\n",
    "\n",
    "\n",
    "# df = weather_df.join(website_df, how=\"inner\", on=\"date\").select(*columns)\n",
    "# partition = Window.partitionBy(\"Address\").orderBy(F.col(\"date\").asc())\n",
    "# df = df.withColumn(\"lag1\", lag(\"y\", 1).over(partition))\n",
    "# df = df.withColumn(\"lag24\", lag(\"y\", 24).over(partition)).drop(\"Address\").na.drop(subset=[\"lag1\", \"lag24\"]).na.fill(0)\n",
    "\n",
    "# columns = [\n",
    "#     \"date\",\n",
    "#     \"Temperature\",\n",
    "#     \"Relative Humidity\",\n",
    "#     \"Wind Speed\",\n",
    "#     \"Visibility\",\n",
    "#     \"lag1\",\n",
    "#     \"lag24\",\n",
    "#     \"y\"]\n",
    "\n",
    "# _df = df.withColumn(\"filter_col\", when(date_format(F.col(\"date\"),\"HH:mm:ss\").between(\"07:00:00\",\"23:00:00\"),\"day\").otherwise(\"night\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Days\n",
    "# df_day = _df.filter(F.col(\"filter_col\")==\"day\").drop(\"filter_F.col\")\n",
    "\n",
    "# # train test split\n",
    "# df_day = df_day.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"date\")))\n",
    "# df_day_train = df_day.where(\"rank <= .8\").select(*columns).sort('date')\n",
    "# df_day_test = df_day.where(\"rank > .8\").select(*columns).sort('date')\n",
    "\n",
    "# # Chain model, assembler and scaler into a Pipeline.\n",
    "# assemblers = [\n",
    "#     VectorAssembler(inputCols=columns[1:-3], outputCol='weather_coditions_vec'),\n",
    "#     VectorAssembler(inputCols=columns[5:-1], outputCol='autoregressive_features_vec'),\n",
    "# ]              \n",
    "\n",
    "# scalers = [\n",
    "#     MinMaxScaler(inputCol=\"weather_coditions_vec\", outputCol=\"weather_coditions_vec_scaled\"),\n",
    "#     MinMaxScaler(inputCol=\"autoregressive_features_vec\", outputCol=\"autoregressive_features_vec_scaled\"),\n",
    "# ]\n",
    "         \n",
    "# pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "# scalerModel = pipeline.fit(df_day_train)\n",
    "# _df_day_train_scaled = scalerModel.transform(df_day_train)\n",
    "# _df_day_test_scaled = scalerModel.transform(df_day_test)\n",
    "\n",
    "# df_day_train_scaled = _df_day_train_scaled.select(\"date\", \"weather_coditions_vec_scaled\", \"lag1\", \"lag24\", \"y\").sort(\"date\")\n",
    "# df_day_test_scaled = _df_day_test_scaled.select(\"date\", \"weather_coditions_vec_scaled\", \"lag1\", \"lag24\", \"y\").sort(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = {x + \"_scaled\": x for x in columns_to_scale}\n",
    "# firstelement=udf(lambda v:float(v[0]),FloatType())\n",
    "\n",
    "# df_day_train_scaled = _df_day_train_scaled.select([firstelement(c).alias(c) for c in _df_day_test_scaled.columns])\n",
    "# df_day_train_scaled = df_day_train_scaled.select([F.col(c).alias(names[c]) for c in names.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
